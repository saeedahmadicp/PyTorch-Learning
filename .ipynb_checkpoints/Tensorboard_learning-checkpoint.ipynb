{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16cfa3da-c0b0-4a9b-a7b9-ccda8192f16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\root\\anaconda3\\envs\\torch-env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose,Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c77d20a-5794-44c1-969c-d5be3970a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose(\n",
    "                    [ToTensor(),\n",
    "                    Normalize((0.5,),(0.5,))]\n",
    "                    )\n",
    "\n",
    "\n",
    "#Download training data from open datasets\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms,\n",
    ")\n",
    "\n",
    "#Download the test data from open datasets\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9bd2ac8-fbfb-4ef6-91e0-3e5358d15863",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "#Create data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = (\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    ")\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img/2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    if one_channel: \n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        ptl.imshow(np.transpose(npimage, (1,2,0)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13e204e5-1d4a-4d5d-8ae4-2406d03cf95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Get cpu or gpu device for training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "#Define model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,5)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Net().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93069df7-2c52-4c40-a025-a12d5ea42791",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2fd5b74-89f5-4579-9705-7842b60f7bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writter = SummaryWriter(\"runs/fashion_mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc60bc0-e282-4b4e-a5da-67b8aabe91ba",
   "metadata": {},
   "source": [
    "## Writing to Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff8c8423-5837-4e39-86bd-5a8fe37300bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get some random training images\n",
    "data_iter = iter(train_dataloader)\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "#create grid of images\n",
    "img_grid = make_grid(images)\n",
    "\n",
    "#show images\n",
    "#matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "#write to tensorboard\n",
    "writter.add_image('four_fasion_mnist_images', img_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "595ae3a2-52e1-462f-9808-29e486ff760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.to(device)\n",
    "writter.add_graph(model, images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b409efbd-3a53-4b73-9672-600d8bedf7da",
   "metadata": {},
   "source": [
    "## Adding a \"Projector\" to Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02199855-f8d0-40a4-9c9c-90348ca88096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "## Helper function\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    \n",
    "    assert len(data) == len(labels)\n",
    "    \n",
    "    perm = torch.randperm(len(data))\n",
    "    \n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "\n",
    "# select random images and their target indices\n",
    "images, labels = select_n_random(training_data.data, training_data.targets)\n",
    "\n",
    "## get the class labels for each image\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "## log embeddings\n",
    "images = images.to(device)\n",
    "features = images.view(-1, 28*28)\n",
    "writter.add_embedding(features, metadata=labels, label_img=images.unsqueeze(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474c69c-c9bd-4639-94f0-5db168dc98f9",
   "metadata": {},
   "source": [
    "## Tracking model training with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48f5f5b9-18cd-4dd6-bed7-0ddf3bdbcd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_probs(model, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probablilities from a tranined network and a list of images\n",
    "    '''\n",
    "    \n",
    "    output = model(images)\n",
    "    \n",
    "    #convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output,1)\n",
    "    preds_tensor2 = preds_tensor.detach().cpu()\n",
    "    preds = np.squeeze(preds_tensor2.numpy())\n",
    "    return preds, [torch.softmax(el, dim=0)[i].item() for i,el in zip(preds, output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da05eace-3dc1-473d-9f87-77f694df16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib figure using a trained network, along with images and labels from a batch, that shows the network's top prediction\n",
    "    along with its probability, alongside the actual label, coloring this information based on whether the prediction was correct or not. \n",
    "    Uses the \"images_to_probs\" function\n",
    "    '''\n",
    "    \n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    fig = plt.figure(figsize=(12,48))\n",
    "    \n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(classes[preds[idx]], probs[idx]*100, classes[labels[idx]]), color=(\"green\" if preds[idx] == labels[idx].item() else \"red\"))\n",
    "       \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66cef268-57e7-4fb6-96da-7a9f03229934",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_dataloader)\n",
    "images, labels = data_iter.next()\n",
    "images = (images).to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "#plot_classes_preds(model, images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47715314-2ca1-48ae-9fee-f5041e96d401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishted training\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "for epoch in range(1):\n",
    "    \n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        \n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss1 = loss(outputs, labels)\n",
    "        loss1.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss1.item()\n",
    "        \n",
    "        if i % 1000 == 999:\n",
    "            \n",
    "            writter.add_scalar('training loss', running_loss/1000, epoch * len(train_dataloader)+i)\n",
    "            \n",
    "            writter.add_figure('prediction vs actual', plot_classes_preds(model, inputs, labels), global_step=epoch*len(train_dataloader)+i)\n",
    "            running_loss = 0.0\n",
    "            \n",
    "print(\"Finishted training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f7c32-08df-4df6-949e-10dcf7ae8756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6d6f612ea1ddb6a51bf90eba8c81a7d5c6dcb74153c25af92fb3c2ec92e831a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
