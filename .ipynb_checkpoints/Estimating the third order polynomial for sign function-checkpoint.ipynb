{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0954a970-3961-4bb0-afcf-b27a87a12371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c062729c-4d6d-4bc7-9b9f-909030e03dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "22e2a2b2-16da-4a07-b676-16e6739271a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype) #The function torch.linspace() returns a one-dimensional tensor of steps equally spaced points between start and end.\n",
    "y = torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "47b16fb3-98f0-4091-9f7c-42eaccd94d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomaly intialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8125283f-6fac-46ff-893f-38cc7c8f8ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2a505112-bb71-42e4-a36e-ddfb92d7cb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 753.1223754882812\n",
      "199 504.23077392578125\n",
      "299 338.703369140625\n",
      "399 228.57781982421875\n",
      "499 155.28262329101562\n",
      "599 106.4803466796875\n",
      "699 73.97210693359375\n",
      "799 52.307899475097656\n",
      "899 37.863548278808594\n",
      "999 28.227968215942383\n",
      "1099 21.796829223632812\n",
      "1199 17.502073287963867\n",
      "1299 14.63230037689209\n",
      "1399 12.713552474975586\n",
      "1499 11.429835319519043\n",
      "1599 10.57037353515625\n",
      "1699 9.994599342346191\n",
      "1799 9.60855484008789\n",
      "1899 9.349535942077637\n",
      "1999 9.175618171691895\n",
      "\n",
      "Result: y = -0.010473142378032207 + 0.8410497903823853 x + 0.0018067918717861176 x^2 + -0.09109847247600555 x^3\n"
     ]
    }
   ],
   "source": [
    "for t in range(2000):\n",
    "    \n",
    "    #forward pass :compute predicted y\n",
    "    y_pred = a + (b*x) + (c*x**2) + (d*x**3)\n",
    "    \n",
    "    #compute and print loss\n",
    "    loss = (y_pred -y).pow(2).sum()\n",
    "    \n",
    "    if t%100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    #use autograd to compute the backward pass. This clall will compute the gradient of loss with respect\n",
    "    #to all Tensors with requires_grad=True. After this call a.grad, b.grad, c.grad and d.grad will be \n",
    "    # Tensor holding the gradient of the loss with respecct to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "   \n",
    "    \n",
    "    #Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    #\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "    \n",
    "        #manually zero the gradients after updating the weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "    \n",
    "print(f\"\\nResult: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca5b9b-049a-4c92-b96b-011c3b349ae6",
   "metadata": {},
   "source": [
    "### Custom Autograd Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bfbcea7a-1909-4a6c-9040-31f1f41441ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing torch.autograd.Function \n",
    "    and implementing the forward and backward passes which operate on Tensors\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return a Tensor\n",
    "        containing the output. xtx is a context object that can be used to stash information\n",
    "        for backward computation. You can cashe arbitrary objects for use in the backward \n",
    "        pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        \n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * ( 5 * input ** 3 - 3*input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss with\n",
    "        respect to the output, and we need to compute the gradient of the loss with respect\n",
    "        to the input\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 0.5 * (15 * input ** 2 - 3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b03d7de9-66cc-4256-a4ec-9b6a56c1c56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 364.7557373046875\n",
      "199 336.39178466796875\n",
      "299 311.39404296875\n",
      "399 288.35430908203125\n",
      "499 267.107421875\n",
      "599 247.50640869140625\n",
      "699 229.41806030273438\n",
      "799 212.721435546875\n",
      "899 197.30628967285156\n",
      "999 183.07151794433594\n",
      "1099 169.92442321777344\n",
      "1199 157.78001403808594\n",
      "1299 146.5602264404297\n",
      "1399 136.19332885742188\n",
      "1499 126.61357116699219\n",
      "1599 117.75997924804688\n",
      "1699 109.57664489746094\n",
      "1799 102.01229858398438\n",
      "1899 95.01947021484375\n",
      "1999 88.55453491210938\n",
      "\n",
      "Result: y = -3.423275984459906e-06 + 1.5930246114730835 x + 4.4686668587701206e-08 x^2 + -0.2502686679363251 x^3\n"
     ]
    }
   ],
   "source": [
    "for t in range(2000):\n",
    "    \n",
    "    \n",
    "    \n",
    "    #To apply our function, we use function.appy method. we alis this as \"P3\"\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "    \n",
    "    #forward pass :compute predicted y\n",
    "    y_pred = a + b * P3(c+d*x)\n",
    "    \n",
    "    #compute and print loss\n",
    "    loss = (y_pred -y).pow(2).sum()\n",
    "    \n",
    "    if t%100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    #use autograd to compute the backward pass. This clall will compute the gradient of loss with respect\n",
    "    #to all Tensors with requires_grad=True. After this call a.grad, b.grad, c.grad and d.grad will be \n",
    "    # Tensor holding the gradient of the loss with respecct to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "   \n",
    "    \n",
    "    #Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    #\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "    \n",
    "        #manually zero the gradients after updating the weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "    \n",
    "print(f\"\\nResult: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b88fbc-b85e-4bac-a6f9-58c12b1767f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d08617e5-61e4-443b-9a4b-2451efd4d838",
   "metadata": {},
   "source": [
    "## In this example we se the nn package to implement our polynomial model network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2eafa401-2849-4ec2-938c-ffa107103248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math\n",
    "\n",
    "#Create Tensors to hold input and outputs\n",
    "x = torch.linspace(-math.pi, math.pi, 3000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so we can \n",
    "# consider it a linear layer neural network. Let's prepare the tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1,2,3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# IN the above code, x.unqueeze(-1) has shape (2000, 1), and p has shape (3,), for this \n",
    "#case, broadcasting semantics will apply to obtain a tensor of shape (2000, 3)\n",
    "\n",
    "# Use the nn package to define our model as a sequence layers. nn.Sequential is a module which contain\n",
    "# other modules, and applies them in sequence to produce its output. The linear module computes from \n",
    "# input using a linear function, and holds internal Tensors for its weight and bias. The Flatten\n",
    "# layer flattens the output of the linear layer to a 1D tensor, to match the shape of 'y'. \n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3,1),\n",
    "    torch.nn.Flatten(0,1)\n",
    ")\n",
    "\n",
    "#The nn Package also contains definitions of popular loss functions: in this case we will use \n",
    "# Mean Squared Error (MSE) as our loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "db4203cd-5e63-4444-bc3e-bfe1177314ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 436.988525390625\n",
      "199 241.79603576660156\n",
      "299 136.55018615722656\n",
      "399 79.78561401367188\n",
      "499 49.15975570678711\n",
      "599 32.630332946777344\n",
      "699 23.705530166625977\n",
      "799 18.884628295898438\n",
      "899 16.279359817504883\n",
      "999 14.870682716369629\n",
      "1099 14.108551025390625\n",
      "1199 13.695989608764648\n",
      "1299 13.472505569458008\n",
      "1399 13.35135269165039\n",
      "1499 13.285618782043457\n",
      "1599 13.249917030334473\n",
      "1699 13.230517387390137\n",
      "1799 13.219962120056152\n",
      "1899 13.214214324951172\n",
      "1999 13.211074829101562\n",
      "2099 13.20936393737793\n",
      "2199 13.208427429199219\n",
      "2299 13.207916259765625\n",
      "2399 13.207633018493652\n",
      "2499 13.207480430603027\n",
      "2599 13.20739459991455\n",
      "2699 13.20734691619873\n",
      "2799 13.20732307434082\n",
      "2899 13.207306861877441\n",
      "2999 13.207298278808594\n",
      "\n",
      "Result: y = -5.291602064971812e-05 + 0.8567584156990051 x + 9.130395483225584e-06 x^2 + -0.09334048628807068 x^3\n"
     ]
    }
   ],
   "source": [
    "for t in range(3000):\n",
    "    \n",
    "    #forward pass :compute predicted y\n",
    "    y_pred = model(xx)\n",
    "    \n",
    "    #compute and print loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if t%100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "        \n",
    "    #Zero the gradients before running the backward passs\n",
    "    model.zero_grad()\n",
    "    \n",
    "    \n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    #update the weights using gradient descent. Each parameter is a Tensor, so we can access its gradients like we did it before\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "            \n",
    "\n",
    "#first layer of the model\n",
    "linear_layer = model[0]\n",
    "    \n",
    " \n",
    "#for linear layer it's parameters are storead as weight and bias    \n",
    "print(f\"\\nResult: y = {linear_layer.bias.item()} + {linear_layer.weight[:,0].item()} x + {linear_layer.weight[:,1].item()} x^2 + {linear_layer.weight[:,2].item()} x^3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff98d1-5ca8-4885-8e78-54b0fbd5dd9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ee49529-362e-4495-ad15-ce00945713d2",
   "metadata": {},
   "source": [
    "### In this example we will use the nn package to define our model, but we will optimize the model using the RMSprop algorithm provided by the `optim` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d7378901-844b-4e96-bbcd-e3e90ff0d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math\n",
    "\n",
    "#Create Tensors to hold input and outputs\n",
    "x = torch.linspace(-math.pi, math.pi, 3000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so we can \n",
    "# consider it a linear layer neural network. Let's prepare the tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1,2,3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# IN the above code, x.unqueeze(-1) has shape (2000, 1), and p has shape (3,), for this \n",
    "#case, broadcasting semantics will apply to obtain a tensor of shape (2000, 3)\n",
    "\n",
    "# Use the nn package to define our model as a sequence layers. nn.Sequential is a module which contain\n",
    "# other modules, and applies them in sequence to produce its output. The linear module computes from \n",
    "# input using a linear function, and holds internal Tensors for its weight and bias. The Flatten\n",
    "# layer flattens the output of the linear layer to a 1D tensor, to match the shape of 'y'. \n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3,1),\n",
    "    torch.nn.Flatten(0,1)\n",
    ")\n",
    "\n",
    "#The nn Package also contains definitions of popular loss functions: in this case we will use \n",
    "# Mean Squared Error (MSE) as our loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f2dd9489-ab7c-4814-ab13-cf5b64d5d389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 27459.169921875\n",
      "199 11208.5712890625\n",
      "299 4106.93603515625\n",
      "399 1621.7874755859375\n",
      "499 1079.236328125\n",
      "599 945.0682373046875\n",
      "699 810.76953125\n",
      "799 657.9105224609375\n",
      "899 505.5042724609375\n",
      "999 368.8593444824219\n",
      "1099 255.0446319580078\n",
      "1199 165.7698211669922\n",
      "1299 99.91158294677734\n",
      "1399 55.59292984008789\n",
      "1499 29.57561492919922\n",
      "1599 17.510820388793945\n",
      "1699 13.816474914550781\n",
      "1799 13.349454879760742\n",
      "1899 13.300216674804688\n",
      "1999 13.297232627868652\n",
      "2099 13.352880477905273\n",
      "2199 13.393037796020508\n",
      "2299 13.36282730102539\n",
      "2399 13.352320671081543\n",
      "2499 13.362598419189453\n",
      "2599 13.366425514221191\n",
      "2699 13.362287521362305\n",
      "2799 13.361160278320312\n",
      "2899 13.362712860107422\n",
      "2999 13.363043785095215\n",
      "\n",
      "Result: y = -0.0005000153323635459 + 0.8573224544525146 x + -0.0005000244709663093 x^2 + -0.09284860640764236 x^3\n"
     ]
    }
   ],
   "source": [
    "for t in range(3000):\n",
    "    \n",
    "    #forward pass :compute predicted y\n",
    "    y_pred = model(xx)\n",
    "    \n",
    "    #compute and print loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if t%100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "        \n",
    "    #Zero the gradients before running the backward passs\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    #optimizers\n",
    "    optimizer.step()\n",
    "       \n",
    "            \n",
    "\n",
    "#first layer of the model\n",
    "linear_layer = model[0]\n",
    "    \n",
    " \n",
    "#for linear layer it's parameters are storead as weight and bias    \n",
    "print(f\"\\nResult: y = {linear_layer.bias.item()} + {linear_layer.weight[:,0].item()} x + {linear_layer.weight[:,1].item()} x^2 + {linear_layer.weight[:,2].item()} x^3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17df261f-8240-4383-acdc-4d2672814db7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
